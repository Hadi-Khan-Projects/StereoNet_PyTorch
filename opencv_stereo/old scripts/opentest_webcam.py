import cv2
import time
import numpy as np

# Initialize parameters for stereo block matching
params = {'minDisparity': 0,
          'numDisparities': 112,
          'blockSize': 3,
          'P1': 0,
          'P2': 0,
          'disp12MaxDiff': 1,
          'uniquenessRatio': 15,
          'speckleWindowSize': 0,
          'speckleRange': 2,
          'preFilterCap': 0,
          'mode': cv2.STEREO_SGBM_MODE_SGBM_3WAY}

# Create a window to display results
cv2.namedWindow('Disparity', cv2.WINDOW_NORMAL)

# Function to update the stereo block matcher when a trackbar moves
def on_trackbar_change(val, param):
    params[param] = val

# Create trackbars for parameters
for param in params:
    cv2.createTrackbar(param, 'Disparity', params[param], 255, lambda val, param=param: on_trackbar_change(val, param))

# Open video capture for both webcams
# Adjust the indices if your webcams are different
left_video = cv2.VideoCapture(1) 
right_video = cv2.VideoCapture(0)

ret, frame = left_video.read()
imageSize = (frame.shape[1], frame.shape[0])

# Load calibration matrices
calibration_dir = "C:/Users/User/Desktop/StereoNet/StereoNet_PyTorch/cv2_implementation/calib_data/"
distL = np.load(calibration_dir + 'distL.npy')
distR = np.load(calibration_dir + 'distR.npy')
mtxL = np.load(calibration_dir + 'mtxL.npy')
mtxR = np.load(calibration_dir + 'mtxR.npy')
R = np.load(calibration_dir + 'R.npy')
T = np.load(calibration_dir + 'T.npy')

# Compute rectification transform for each stereo head
R1, R2, P1, P2, Q, validPixROI1, validPixROI2 = cv2.stereoRectify(mtxL, distL, mtxR, distR, imageSize, R, T)

# Compute the mapping matrices
map1L, map2L = cv2.initUndistortRectifyMap(mtxL, distL, R1, P1, imageSize, cv2.CV_16SC2)
map1R, map2R = cv2.initUndistortRectifyMap(mtxR, distR, R2, P2, imageSize, cv2.CV_16SC2)

while True:
    start = time.time()

    # Read frames from both webcams
    left_ret, left_frame = left_video.read()
    right_ret, right_frame = right_video.read()

    # If frames were successfully read
    if left_ret and right_ret:
        # Apply the rectification
        left_frame_rectified = cv2.remap(left_frame, map1L, map2L, cv2.INTER_LINEAR)
        right_frame_rectified = cv2.remap(right_frame, map1R, map2R, cv2.INTER_LINEAR)

        # Convert both frames to grayscale
        left_gray = cv2.cvtColor(left_frame_rectified, cv2.COLOR_BGR2GRAY)
        right_gray = cv2.cvtColor(right_frame_rectified, cv2.COLOR_BGR2GRAY)

        # Create a stereo block matcher with current parameters
        stereo = cv2.StereoSGBM_create(
            minDisparity=params['minDisparity'],
            numDisparities=params['numDisparities'],
            blockSize=params['blockSize'],
            P1=params['P1'],
            P2=params['P2'],
            disp12MaxDiff=params['disp12MaxDiff'],
            uniquenessRatio=params['uniquenessRatio'],
            speckleWindowSize=params['speckleWindowSize'],
            speckleRange=params['speckleRange'],
            preFilterCap=params['preFilterCap'],
            mode=params['mode']
        )

        # Calculate disparity (depth map)
        disparity = stereo.compute(left_gray, right_gray).astype(np.float32) / 16.0

        # Normalize disparity to the range 0-255 for viewing
        disparity = cv2.normalize(disparity, disparity, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)

        # Display disparity
        cv2.imshow('Disparity', disparity)

        # Break loop if 'q' is pressed
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

        # Print frames per second
        print("FPS: ", 1.0 / (time.time() - start))

# When everything done, release the video capture and close windows
left_video.release()
right_video.release()
cv2.destroyAllWindows()